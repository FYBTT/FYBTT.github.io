---
layout:     post
title:      "贝叶斯思维——chapter4(估计进阶)"
author:     bear
header-img: img/books.jpg
catalog:    true
categories: 贝叶斯思维
tags:       python
---

# 4.1 欧元问题

>《信息论：推理和学习算法》中，有过这样一个问题：“当硬币以边缘转动250次,得到正面140次，反面110次。”，“统计学声明称：如果这是一个均匀的硬币，这样的结果出现的可能性小于7%。”

上述结果是否对“硬币偏心而非均匀”提供了证据？

## 解：

我们将采取一下步骤来回答这个问题。

 1. 估计该硬币正面朝上的概率。
 2. 估计该数是否支持硬币偏心的假设。
 
已知一个硬币，那么我们以其边缘转动，它正面朝上的概率都是确定的$x$。（取决于硬币的物理特性——重量分布）
如果硬币的重量分布是均匀的，那么我们认为这里的$x$接近$50\%$，但是作为一个不均匀的硬币，这个$x$应该会有较大差别。

接下来要利用贝叶斯定理和观察到的数据来估计$x$。
我们先定义假设$H_x$，表示正面朝上的概率为$x\%$:

```python
hypos = range(1,101)
```

先从均匀的先验概率开始。
Likelihood的函数相对容易，若$H_x$为真，正面朝上概率为$x/100$，反面朝上概率为$(1 - x/100)$。

```python
class Euro(Suite):
        
    def likelihood(self, data, hypo):
        x = hypo
        if data == 'H'
            return x/100.0
        return 1 - x/100.0
```
利用观察到的数据对x进行估计。
```python
suite  = Euro(hypos)
dataSet = 'H'*140+'T'*110
for data in dataSet:
    suite.Update(data)
```
![此处输入图片的描述][1]

# 4.2 后验概率的概述
总结一下，有几种方式来概括后验概率分布的特征。

 1. 找后验概率的最大似然。（你会发现这里的最大似然$val=56$，实际上就等于观察得到的百分比$140/250$。
 ```python
 def MaximumLikelihood(pmf):
    #返回最高概率的值
    prob, val = max((prob, val) for val, prob in pmf.Items())
    return val
 ```
 2. 计算平均数($期望55.95$)和中位数($56$)来概述后验概率。
 3. 计算置信区间$(51, 61)$来概述后验概率$90\%$。

回到原来的问题，我们想知道硬币是否是均匀的。**当观察到的后验可信区间不包括50％，我们认为硬币的重量分布的确是不均匀的**。

但确切地说，这不是我们最开始的问题。之前的问题是“**这些数据是否恰恰为——硬币偏心而非均匀——给出了证据？** “要回答这个问题，我们需要更加精确得理解‘数据为某假说提供证据’这句话的含义，这个放到下一章讲。

既然我们想要知道硬币是否是均匀的，很自然得想到是求$x$为$50\%$的概率：
```python
suite.Prob(50)
>>>0.021
```
然而这个值几乎不能说明什么，这使得对假设的评估显得毫无意义。

# 4.3 先验概率的湮没
我们之前假设先验是均匀的，但这可能不是一个好的假设。如果硬币是偏心的，我们可以相信x会大幅度偏离$50\%$，但如果偏心到$10\%$或者$90\%$就很扯。
更合理的方式，是为$50%$附近给出一个相对与$10%$或者$90%$更高的先验概率。
我们可以构建一个三角先验分布的例子：
![此处输入图片的描述][2]
 只是给力一个先验分布的形状，还需要做归一化。
 下面的代码构成了所谓的三角先验分布：
```python
 def TrianglePrior():
    suite = Euro
    for x in range(0,51):
        suite.Set(x,x)
    for x in range(51,101):
        suite.Set(x,100-x);
    suite.Normalize()
```
事实上，即使我们应用了不同的先验，后验分布也非常得相似。
这就是先验湮没的一个例子：如果有足够的数据，计时在先验分布上持有不同的观点，我们也会得到趋于**收敛的后验概率**。

# 4.4 优化
从上述的代码我们可以看到，对于每个得到的数据我们都会单独的应用一次Update,从上一章的Update的实现中我们可以看到在每次应用一次该函数，都需要进行一次归一化处理。

如果我们已知了一个数据的序列，就没有必要一个一个得进行训练，对所有数据都训练结束了，再进行归一化即可。
```python
def UpdateSet(self, dataSet):
    for data in dataSet:
        for hypo in self.Values():
            like = self.Likelihood(data, hypo)
            self.Mult(hypo, like)
    return self.Normalize()
```
这样处理会在一定程度上优化我们的速度，但time依然与数据量成正比。

我们可以通过改写Likelihood来处理整个数据集，而不是一次模拟运行一次。
对于之前的硬币版本改动如下：
```python
def Likelihood(self, data, hypo):
    x = hypo/100.0
    heads, tails = data
    like = x**heads * (1 - x)**tails
    return like
```
然后我们就可以用两个整数的元组来调用Update:
```python
heads, tails = 140, 110
suite.Update((heads, tails))
```
我们用指数函数的计算取代了原来的乘法，这样对于任意次数的硬币模拟消耗的运行时间基本一致。

# 4.5 Beta分布
还有一个优化可以让解法更快。
我们在4.3节的时候讨论过先验淹没的问题。可以假设硬币抛掷为正面的先验概率满足Beta分布（连续分布）。

Beta分布是一种连续型概率密度分布，表示为 $ Beta(\alpha , \beta) $，由两个参数 $\alpha 和 \beta$ 决定，称为形状参数。
由于其定义域为$(0,1)$，**一般被用于建模伯努利试验事件成功的概率的概率分布**：
![此处输入图片的描述][3]

>对于硬币或者骰子这样的简单实验，我们事先能很准确地掌握系统成功的概率。
然而通常情况下，系统成功的概率是未知的，但是根据频率学派的观点，我们可以通过频率来估计概率。为了测试系统的成功概率，我们做n次试验，统计成功的次数k，于是很直观地就可以计算出。
然而由于系统成功的概率是未知的，这个公式计算出的只是系统成功概率的最佳估计。也就是说实际上也可能为其它的值，只是为其它的值的概率较小。因此我们并不能完全确定硬币出现正面的概率就是该值，所以也是一个随机变量，它符合Beta分布，其取值范围为0到1

用一句话来说，beta分布可以看作一个概率的概率密度分布，当你不知道一个东西的具体概率是多少时，它可以给出了所有概率出现的可能性大小。

前文提到进行n次伯努利试验，其出现试验成功的概率p服从一个先验概率密度分布 $ Beta(\alpha , \beta) $，试验结果出现k次试验正面，则试验成功的概率p的后验概率密度分布为 $ Beta(\alpha + k , \beta + n - k) $ 。

$$
\begin{align}
f(x;\alpha,\beta) &= \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1}\qquad \text{beta 分布} \\
f(x;\alpha,\beta) &\propto x^{\alpha-1}(1-x)^{\beta-1}\\
\Gamma(\alpha) &= \int_0^{\infty}t^{\alpha-1}e^{-t}\,dt\qquad \text{gamma 分布}
\end{align}
$$

thinkbayes.py提供了一个类来表示beta分布：
```python
class Beta(object):
    def __init__(self, alpha = 1, beta = 1):
        self.alpha = alpha
        self.beta = beta
```
默认情况下，```__init__```使用均匀分布。Update进行贝叶斯更新：
```python
    def Update(self, data):
        heads, tails = data
        self.alpha += heads
        self.beta += tails
```
data是一对表示正面和反面的数量的整数。
因此，我们的得到了另一种解决欧元问题的方法：

```python
beta = thinkbayes.Beta()
beta.Update((140, 110))
print beta.Mean()
```

beta提供了Mean，计算期望的函数：

```python
def Mean(self):
    return float(self.alpha) / (self.alpha + self.beta)
```
# 4.6 讨论
我们可以看到，在较大数据集的情况下，先验之间的区别被掩盖了，这会减轻一些我们在前面一张关于客观性的担忧。甚至很对现实世界的问题，明显不同的先验概率最终会被矫正。
但事实并不总是如此。首先，请记住，所有贝叶斯分析是基于模型决策的。如果两个人没有选择相同的模型，我们可能对数据进行不同的解读。因此，即使使用相同的数据，我们也可能得到不同的似然度，因而后验概率可能不会收敛。
另外，请注意，在贝叶斯Update中，我们以一个似然度乘以每个先验概率，所以如果$p(H)$为0，则$p(H|D)$也为0。在欧元问题上，如果我们确定x小于50%，并指定大于50%的假设概率都为0，那么再多的数据都无法说服你。
这种看法是**克伦威尔法则**的基础:
>应当避免设置任何一个假设的先验概率为0，哪怕的确存在这种可能性。

[1]: /img/bayies/4/train1.png
[2]: /img/bayies/4/dis.png
[3]: /img/bayies/4/beta.png


